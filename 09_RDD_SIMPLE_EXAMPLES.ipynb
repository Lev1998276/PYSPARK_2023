{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMzwNua8kq0-",
        "outputId": "071f793f-8752-4d95-8a45-203160e5c68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=48d9bf8d60cc4675312672dc5948ae0ba8600fb1fb7b102eacc2c276846a0b41\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART1"
      ],
      "metadata": {
        "id": "as9YI2D4uAD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"RDDMapTransformation\").getOrCreate()\n",
        "\n",
        "# Create an RDD\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Apply a map transformation to double each element\n",
        "z_add = lambda x, y : x + y\n",
        "z_multiply = lambda x,y : x * y\n",
        "z_divide = lambda x, y : x / y\n",
        "z_filter = lambda x :  x % 2 == 0\n",
        "z_square = lambda x : x * 2\n",
        "\n",
        "mapped_rdd = rdd.reduce(z_add)\n",
        "multiply_rdd = rdd.reduce(z_multiply)\n",
        "divide_rdd = rdd.reduce(z_divide)\n",
        "filter_rdd = rdd.map(z_filter)\n",
        "square_rdd = rdd.map(z_square)\n",
        "\n",
        "\n",
        "print(mapped_rdd)\n",
        "print(multiply_rdd)\n",
        "print(divide_rdd)\n",
        "print(filter_rdd.collect())\n",
        "print(square_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gSBT5Dzmvsf",
        "outputId": "aef96768-63d2-40c7-b511-cec490135081"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "120\n",
            "3.3333333333333335\n",
            "[False, True, False, True, False]\n",
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART2**"
      ],
      "metadata": {
        "id": "w0mVvIY-uFxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [\"Hello World\", \"PySpark is great\"]\n",
        "rdd2 = spark.sparkContext.parallelize(data2)\n",
        "\n",
        "z_split = lambda x: x.split(\" \")\n",
        "\n",
        "\n",
        "flatmapped_rdd = rdd2.flatMap(z_split)\n",
        "result_fmapped = flatmapped_rdd.collect()\n",
        "print(result_fmapped)\n",
        "\n",
        "\n",
        "#count by value\n",
        "text_rdd = spark.sparkContext.parallelize([\"Hello Spark. Spark is powerful.\", \"Hello PySpark.\"])\n",
        "\n",
        "words_rdd = text_rdd.flatMap(z_split)\n",
        "result_split_count = words_rdd.countByValue()\n",
        "result_split_count\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNdnISp5oM_f",
        "outputId": "c0e3c4c9-0731-4707-ec45-19bb86ca308d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'World', 'PySpark', 'is', 'great']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'Hello': 2,\n",
              "             'Spark.': 1,\n",
              "             'Spark': 1,\n",
              "             'is': 1,\n",
              "             'powerful.': 1,\n",
              "             'PySpark.': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART3**"
      ],
      "metadata": {
        "id": "jHi_xqZhuTt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "data3 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd3 = spark.sparkContext.parallelize(data3)\n",
        "\n",
        "# Filter even numbers and square each element\n",
        "filtered_and_squared_rdd = rdd.filter(lambda x: x % 2 == 0).map(lambda x: x ** 2)\n",
        "filtered_and_squared_rdd.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIg-D0qhuVem",
        "outputId": "a4206588-d808-4e6b-ee6f-54bab4d8bb6c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"RDDExamples\").getOrCreate()\n",
        "\n",
        "# Create an RDD with words\n",
        "data = [\"Hello\", \"Spark\", \"Hello\", \"PySpark\", \"Spark\", \"PySpark\", \"PySpark\"]\n",
        "words_rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Create a Pair RDD and perform reduceByKey for word count\n",
        "word_counts_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect and print the results\n",
        "result = word_counts_rdd.collect()\n",
        "print(result)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHGIy97euvsW",
        "outputId": "f9c923da-4a78-4f9f-da22-61d202db5b80"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 2), ('Spark', 2), ('PySpark', 3)]\n"
          ]
        }
      ]
    }
  ]
}