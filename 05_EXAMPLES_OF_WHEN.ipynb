{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kZKIxnjTziT",
        "outputId": "b9288c41-e1f7-44a1-a508-0788e749853e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=22079246624ef8266d23ff8a66f9c65cb54d9011362a7db8f3ce4087d63ba3f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhenExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 25),\n",
        "        (\"Bob\", 30),\n",
        "        (\"Charlie\", 22),\n",
        "        (\"David\", None)]\n",
        "\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Use the when function for conditional transformation\n",
        "result_df = df.withColumn(\"Age_Group\",\n",
        "                          when(col(\"Age\").isNull(), \"Unknown\")\n",
        "                          .when(col(\"Age\") < 25, \"Young\")\n",
        "                          .when((col(\"Age\") >= 25) & (col(\"Age\") <= 30), \"Adult\")\n",
        "                          .otherwise(\"Senior\"))\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFSccSPyD8ZP",
        "outputId": "2657c551-1284-48c1-c443-06b2ea72dace"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|   Name| Age|\n",
            "+-------+----+\n",
            "|  Alice|  25|\n",
            "|    Bob|  30|\n",
            "|Charlie|  22|\n",
            "|  David|NULL|\n",
            "+-------+----+\n",
            "\n",
            "+-------+----+---------+\n",
            "|   Name| Age|Age_Group|\n",
            "+-------+----+---------+\n",
            "|  Alice|  25|    Adult|\n",
            "|    Bob|  30|    Adult|\n",
            "|Charlie|  22|    Young|\n",
            "|  David|NULL|  Unknown|\n",
            "+-------+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhenExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 25),\n",
        "        (\"Bob\", 30),\n",
        "        (\"Charlie\", 22),\n",
        "        (\"David\", 55),\n",
        "        (\"Eva\", 65)]\n",
        "\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# # Use the when function to categorize age groups\n",
        "# result_df = df.withColumn(\"Age_Group\",\n",
        "#                           when(col(\"Age\") < 18, \"Child\")\n",
        "#                           .when((col(\"Age\") >= 18) & (col(\"Age\") < 65), \"Adult\")\n",
        "#                           .otherwise(\"Senior\"))\n",
        "\n",
        "# # Show the result\n",
        "# result_df.show()\n",
        "\n",
        "#my way\n",
        "result_df =  df.withColumn(\"Age_Class\", when(col(\"Age\") <18, \"Child\" )\n",
        "                                      .when((col(\"Age\") >=18) & (col(\"Age\") < 55), \"Adult\")\n",
        "                                      .when(col(\"Age\") >= 65,\"Senior\" )\n",
        "                                      .otherwise(\"Cannot be classified\"))\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbL-u5kENFR",
        "outputId": "22bfc3dd-55f7-4806-b464-407477c0c76e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+--------------------+\n",
            "|   Name|Age|           Age_Class|\n",
            "+-------+---+--------------------+\n",
            "|  Alice| 25|               Adult|\n",
            "|    Bob| 30|               Adult|\n",
            "|Charlie| 22|               Adult|\n",
            "|  David| 55|Cannot be classified|\n",
            "|    Eva| 65|              Senior|\n",
            "+-------+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhenExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 50000),\n",
        "        (\"Bob\", 75000),\n",
        "        (\"Charlie\", 90000),\n",
        "        (\"David\", 120000),\n",
        "        (\"Eva\", 180000)]\n",
        "\n",
        "columns = [\"Name\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# # Use the when function to assign a status based on salary\n",
        "# result_df = df.withColumn(\"Salary_Status\",\n",
        "#                           when(col(\"Salary\") < 60000, \"Low\")\n",
        "#                           .when((col(\"Salary\") >= 60000) & (col(\"Salary\") < 100000), \"Medium\")\n",
        "#                           .otherwise(\"High\"))\n",
        "\n",
        "# Show the result\n",
        "\n",
        "print(\"Raw Data\")\n",
        "df.show()\n",
        "\n",
        " #  .when((col(\"Salary\") > 60000) & (col(\"Salary\") < 100000), \"Medium\")\n",
        "\n",
        "result_df = df.withColumn(\"Salary\",\n",
        "                          when((col(\"Salary\") < 60000), \"Low\")\n",
        "                          .when((col(\"Salary\") >= 60000) & (col(\"Salary\") < 100000), \"Medium\")\n",
        "                          .otherwise(\"High\"))\n",
        "\n",
        "\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inN61H32GbYz",
        "outputId": "f53498cf-7ea1-4ea6-f942-a4f21c61c92d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data\n",
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|  Alice| 50000|\n",
            "|    Bob| 75000|\n",
            "|Charlie| 90000|\n",
            "|  David|120000|\n",
            "|    Eva|180000|\n",
            "+-------+------+\n",
            "\n",
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|  Alice|   Low|\n",
            "|    Bob|Medium|\n",
            "|Charlie|Medium|\n",
            "|  David|  High|\n",
            "|    Eva|  High|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhenWithAggregates\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 25, \"Group1\"),\n",
        "        (\"Bob\", 30, \"Group1\"),\n",
        "        (\"Charlie\", 22, \"Group2\"),\n",
        "        (\"David\", 28, \"Group2\"),\n",
        "        (\"Eva\", 35, \"Group1\")]\n",
        "\n",
        "columns = [\"Name\", \"Age\", \"Group\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Raw Data\")\n",
        "df.show()\n",
        "\n",
        "avg_age_under_30 = avg(when(col(\"Age\") < 30, col(\"Age\"))).alias(\"avg_age_under_30\")\n",
        "avg_age_30_and_over = avg(when(col(\"Age\") >= 30 , col(\"Age\"))).alias(\"avg_age_30_and_over\")\n",
        "\n",
        "result_df = df.groupBy(\"Group\")\\\n",
        "              .agg(avg_age_under_30,\n",
        "                   avg_age_30_and_over)\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayTAKiavG9bk",
        "outputId": "8210097b-8096-4c5a-c030-92ac8e724954"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data\n",
            "+-------+---+------+\n",
            "|   Name|Age| Group|\n",
            "+-------+---+------+\n",
            "|  Alice| 25|Group1|\n",
            "|    Bob| 30|Group1|\n",
            "|Charlie| 22|Group2|\n",
            "|  David| 28|Group2|\n",
            "|    Eva| 35|Group1|\n",
            "+-------+---+------+\n",
            "\n",
            "+------+----------------+-------------------+\n",
            "| Group|avg_age_under_30|avg_age_30_and_over|\n",
            "+------+----------------+-------------------+\n",
            "|Group1|            25.0|               32.5|\n",
            "|Group2|            25.0|               NULL|\n",
            "+------+----------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, count\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhenWithAggregates\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", \"Yes\"),\n",
        "        (\"Bob\", \"No\"),\n",
        "        (\"Charlie\", \"Yes\"),\n",
        "        (\"David\", \"No\"),\n",
        "        (\"Eva\", \"Yes\")]\n",
        "\n",
        "columns = [\"Name\", \"Has_Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"RawData\")\n",
        "df.show()\n",
        "\n",
        "count_yes = count(when(col(\"Has_Salary\") == \"Yes\", 1)).alias(\"count_yes\")\n",
        "count_no = count(when(col(\"Has_Salary\") == \"No\", 0)).alias(\"count_no\")\n",
        "\n",
        "result_df = df.groupBy(\"Has_Salary\").agg(count_yes,count_no)\n",
        "result_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufxxp3gBM5Li",
        "outputId": "17c4cc7e-69f4-47eb-fde2-16b11f818ae5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RawData\n",
            "+-------+----------+\n",
            "|   Name|Has_Salary|\n",
            "+-------+----------+\n",
            "|  Alice|       Yes|\n",
            "|    Bob|        No|\n",
            "|Charlie|       Yes|\n",
            "|  David|        No|\n",
            "|    Eva|       Yes|\n",
            "+-------+----------+\n",
            "\n",
            "+----------+---------+--------+\n",
            "|Has_Salary|count_yes|count_no|\n",
            "+----------+---------+--------+\n",
            "|        No|        0|       2|\n",
            "|       Yes|        3|       0|\n",
            "+----------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"ComplexWhenExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", \"Excellent\", 50000),\n",
        "        (\"Bob\", \"Good\", 75000),\n",
        "        (\"Charlie\", \"Poor\", 90000),\n",
        "        (\"David\", \"Excellent\", 120000),\n",
        "        (\"Eva\", \"Good\", 180000)]\n",
        "\n",
        "columns = [\"Name\", \"Performance\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Raw Data\")\n",
        "df.show()\n",
        "\n",
        "\n",
        "result_df = df.withColumn(\"Adjusted_Salary\",\n",
        "                           when(col(\"Performance\") == \"Excellent\", col(\"Salary\") * 1.2)\\\n",
        "                           .when(col(\"Performance\") == \"Good\", col(\"Salary\") * 1.1)\\\n",
        "                           .when(col(\"Performance\") == \"Poor\", col(\"Salary\") * 0.9)\\\n",
        "                           .otherwise(col(\"Salary\") * -1))\n",
        "\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE9ejvwUN4Wk",
        "outputId": "7a9971a5-900c-4e1e-b9e1-8a208f78a453"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data\n",
            "+-------+-----------+------+\n",
            "|   Name|Performance|Salary|\n",
            "+-------+-----------+------+\n",
            "|  Alice|  Excellent| 50000|\n",
            "|    Bob|       Good| 75000|\n",
            "|Charlie|       Poor| 90000|\n",
            "|  David|  Excellent|120000|\n",
            "|    Eva|       Good|180000|\n",
            "+-------+-----------+------+\n",
            "\n",
            "+-------+-----------+------+------------------+\n",
            "|   Name|Performance|Salary|   Adjusted_Salary|\n",
            "+-------+-----------+------+------------------+\n",
            "|  Alice|  Excellent| 50000|           60000.0|\n",
            "|    Bob|       Good| 75000|           82500.0|\n",
            "|Charlie|       Poor| 90000|           81000.0|\n",
            "|  David|  Excellent|120000|          144000.0|\n",
            "|    Eva|       Good|180000|198000.00000000003|\n",
            "+-------+-----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"ComplexWhenExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 500, 5),\n",
        "        (\"Bob\", 750, 15),\n",
        "        (\"Charlie\", 900, 8),\n",
        "        (\"David\", 1200, 25),\n",
        "        (\"Eva\", 1800, 35)]\n",
        "\n",
        "columns = [\"Customer\", \"Total_Spending\", \"Total_Purchases\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Raw Data\")\n",
        "df.show()\n",
        "\n",
        "# # Use when with multiple conditions to categorize customers based on purchase behavior\n",
        "# result_df = df.withColumn(\"Customer_Category\",\n",
        "#                           when((col(\"Total_Purchases\") > 20) & (col(\"Total_Spending\") > 1000), \"High Value\")\n",
        "#                           .when((col(\"Total_Purchases\") > 10) & (col(\"Total_Spending\") > 500), \"Medium Value\")\n",
        "#                           .otherwise(\"Low Value\"))\n",
        "\n",
        "# # Show the result\n",
        "# result_df.show()\n",
        "\n",
        "result_df = df.withColumn(\"Customer_Category\",\n",
        "                          when((col(\"Total_Purchases\") > 20) & (col(\"Total_Spending\") > 1000), \"High Value\")\n",
        "                          .when((col(\"Total_Purchases\") > 10) & (col(\"Total_Spending\") > 500), \"Medium Value\")\n",
        "\t\t\t\t\t\t  .otherwise(\"Low Value\"))\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGtX4bduOu-Z",
        "outputId": "c69b57be-c982-4ca7-ebf9-994261c85ef8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data\n",
            "+--------+--------------+---------------+\n",
            "|Customer|Total_Spending|Total_Purchases|\n",
            "+--------+--------------+---------------+\n",
            "|   Alice|           500|              5|\n",
            "|     Bob|           750|             15|\n",
            "| Charlie|           900|              8|\n",
            "|   David|          1200|             25|\n",
            "|     Eva|          1800|             35|\n",
            "+--------+--------------+---------------+\n",
            "\n",
            "+--------+--------------+---------------+-----------------+\n",
            "|Customer|Total_Spending|Total_Purchases|Customer_Category|\n",
            "+--------+--------------+---------------+-----------------+\n",
            "|   Alice|           500|              5|        Low Value|\n",
            "|     Bob|           750|             15|     Medium Value|\n",
            "| Charlie|           900|              8|        Low Value|\n",
            "|   David|          1200|             25|       High Value|\n",
            "|     Eva|          1800|             35|       High Value|\n",
            "+--------+--------------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}